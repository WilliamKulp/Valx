{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:05:37.406628Z",
     "start_time": "2020-11-26T11:05:35.722163Z"
    }
   },
   "outputs": [],
   "source": [
    "import W_utility.file as ufile\n",
    "from W_utility.log import ext_print\n",
    "import os,sys,re\n",
    "import Valx_core\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:05:37.449724Z",
     "start_time": "2020-11-26T11:05:37.408615Z"
    },
    "code_folding": [
     87,
     205
    ]
   },
   "outputs": [],
   "source": [
    "# Valx: A system for extracting and structuring numeric lab test comparison statements from text\n",
    "# Created by Tony HAO, th2510@columbia.edu\n",
    "# Please kindly cite the paper: Tianyong Hao, Hongfang Liu, Chunhua Weng. Valx: A system for extracting and structuring numeric lab test comparison statements from text. Methods of Information in Medicine. Vol. 55: Issue 3, pp. 266-275, 2016\n",
    "\n",
    "debug=False\n",
    "\n",
    "def extract_variables (trials,fdin, ffea, ffea2, var):\n",
    "    # read input data\n",
    "#     if fdin is None or fdin ==\"\": return False\n",
    "#     trials = pd.read_csv(fdin,header=None)\n",
    "    if trials is None or len(trials) <= 0:\n",
    "        print(ext_print ('input data error, please check either no such file or no data --- interrupting'))\n",
    "        return False\n",
    "    print(ext_print ('found a total of %d data items' % len(trials)))\n",
    "    \n",
    "    # read feature list - domain knowledge\n",
    "    if ffea is None or ffea ==\"\": return False\n",
    "    fea_dict_dk = ufile.read_csv_as_dict_with_multiple_items (ffea)\n",
    "    if fea_dict_dk is None or len(fea_dict_dk) <= 0:\n",
    "        print(ext_print ('no feature data available --- interrupting'))\n",
    "        return False\n",
    "\n",
    "    # get feature info\n",
    "    features, feature_dict_dk = {}, {}\n",
    "    if var == \"All\":\n",
    "        features = fea_dict_dk\n",
    "        del features[\"Variable name\"]\n",
    "    elif var in fea_dict_dk:\n",
    "        features = {var:fea_dict_dk[var]}\n",
    "    for key, value in fea_dict_dk.items():\n",
    "        names = value[0].lower().split('|')\n",
    "        for name in names:\n",
    "            if name.strip() != '': feature_dict_dk[name.strip()] =key\n",
    "\n",
    "    # read feature list - UMLS (can be replaced by full UMLS)\n",
    "    if ffea2 is None or ffea2 ==\"\": return False\n",
    "    fea_dict_umls = ufile.read_csv_as_dict (ffea2)\n",
    "    if fea_dict_umls is None or len(fea_dict_umls) <= 0:\n",
    "        print(ext_print ('no feature data available --- interrupting'))\n",
    "        return False\n",
    "\n",
    "    #load numeric feature list\n",
    "    Valx_core.init_features()\n",
    "\n",
    "    output = []\n",
    "    for i in tqdm_notebook(range(0,len(trials))):\n",
    "        if i%1000 == 0:\n",
    "            print ('processing %d' % i)\n",
    "        # pre-processing eligibility criteria text\n",
    "        text = Valx_core.preprocessing(trials.iloc[i,1]) # trials[i][1] is the eligibility criteria text\n",
    "        if debug: print(text)\n",
    "        (sections_num, candidates_num) = Valx_core.extract_candidates_numeric(text) # extract candidates containing numeric features\n",
    "        for j in range(0,len(candidates_num)): # for each candidate\n",
    "            if debug: print(f\"Criteria {j} : {text}\")\n",
    "            exp_text = Valx_core.formalize_expressions(candidates_num[j]) # identify and formalize values\n",
    "            if debug: print(f\"formalize_expressions 1 {j} : {exp_text}\")\n",
    "            (exp_text, key_ngrams) = Valx_core.identify_variable(exp_text, feature_dict_dk, fea_dict_umls) # identify variable mentions and map them to names\n",
    "            if debug: print(f\"formalize_expressions 2 {j} : {exp_text}\")\n",
    "            if debug: print(f\"key_ngrams {j} : {key_ngrams}\")\n",
    "            (variables, vars_values) = Valx_core.associate_variable_values(exp_text)\n",
    "            if debug: print(f\"variables {j} : {variables}\")\n",
    "            if debug: print(f\"vars_values {j} : {vars_values}\")\n",
    "#             print(variables,vars_values)\n",
    "            all_exps = []\n",
    "            for k in range(0,len(variables)):\n",
    "                curr_var = variables[k]\n",
    "                curr_exps = vars_values[k]\n",
    "                if curr_var in features:\n",
    "                    fea_list = features[curr_var]\n",
    "                    curr_exps = Valx_core.context_validation(curr_exps, fea_list[1], fea_list[2])                           \n",
    "                    curr_exps = Valx_core.normalization(fea_list[3], curr_exps) # unit conversion and value normalization\n",
    "                    curr_exps = Valx_core.hr_validation (curr_exps, float(fea_list[4]), float(fea_list[5])) # heuristic rule-based validation\n",
    "                if len(curr_exps) > 0:\n",
    "                    if var == \"All\" or var.lower() == curr_var.lower() or var.lower() in curr_var.lower(): \n",
    "                        all_exps += curr_exps                     \n",
    "#                 print(curr_var)\n",
    "#                 print(curr_exps)\n",
    "            if len(all_exps) > 0: \n",
    "                output.append((trials.iloc[i,0], sections_num[j], candidates_num[j], exp_text, str(all_exps).replace(\"u'\", \"'\"))) # output result\n",
    "#         break\n",
    "    # output result\n",
    "    fout = os.path.splitext(fdin)[0] + \"_exp_%s_out.csv\" % var\n",
    "    pd.DataFrame(output).to_csv(fout,index=None)\n",
    "    print(ext_print ('saved processed results into: %s' % fout))\n",
    "    return output,trials.values\n",
    "\n",
    "\n",
    "def process_valx_results(original_text, valx_outputs) : \n",
    "\n",
    "    word_blocks = get_words_space_blocks(original_text)\n",
    "\n",
    "    all_words = [word_block['word'] for word_block in word_blocks]\n",
    "\n",
    "    count_word_blocks = len(word_blocks)\n",
    "\n",
    "    word_block_index = 0\n",
    "\n",
    "    result = [] \n",
    "    \n",
    "    for output in valx_outputs : \n",
    "\n",
    "        value_exps = output[4]\n",
    "        value_exps = eval(value_exps)\n",
    "\n",
    "        for value_exp in value_exps :\n",
    "\n",
    "            value = value_exp[2]\n",
    "            unit = value_exp[3]\n",
    "            value_type = value_exp[0]\n",
    "\n",
    "            float_count = all_words.count(str(value))\n",
    "            int_count = all_words.count(str(int(value)))\n",
    "            value_count =  float_count + int_count\n",
    "\n",
    "            print(value_exp)\n",
    "            print('value_count', value_count)\n",
    "            print(word_block_index)\n",
    "            if len(result)>0 : \n",
    "                if result[-1]['EntityType'] == value_type and (str(int(value) in get_alphanumeric_groups(result[-1]['Entity'])) or str(value) in get_alphanumeric_groups(result[-1]['Entity'])) : \n",
    "                    continue\n",
    "\n",
    "            if word_block_index == count_word_blocks : \n",
    "                break \n",
    "\n",
    "            elif value_count == 1 :\n",
    "                if float_count == 1 : \n",
    "                    word_block_index = all_words.index(str(value))\n",
    "                else : \n",
    "                    word_block_index = all_words.index(str(int(value)))\n",
    "\n",
    "                if word_block_index <= count_word_blocks - len(unit.split(\" \")) - 2 : \n",
    "                    word = word_blocks[word_block_index][\"word\"]\n",
    "                    word_start_index = word_blocks[word_block_index]['start_index']\n",
    "\n",
    "                    next_word_blocks = word_blocks[word_block_index+1:word_block_index+len(unit.split(\" \"))+1]\n",
    "                    unit_word = \" \".join([word_block['word'] for word_block in next_word_blocks])\n",
    "                    if unit_word == unit : \n",
    "                        if len(next_word_blocks) == 0 : \n",
    "                            end_index = word_end_index\n",
    "                        else : \n",
    "                            end_index = next_word_blocks[-1]['end_index']\n",
    "                        result.append({'Entity':\" \".join([word, unit]), \n",
    "                                       \"EntityType\":value_type, \n",
    "                                       \"StartIndex\":word_start_index,\n",
    "                                       \"EndIndex\":end_index, \n",
    "                                       \"Confidence\":1})\n",
    "                        word_block_index = word_block_index + len(unit.split(\" \")) + 1 \n",
    "\n",
    "                    else : \n",
    "                        result.append({'Entity':word_blocks[word_block_index]['word'], \n",
    "                               'EntityType':value_type, \n",
    "                               'StartIndex':word_blocks[word_block_index]['start_index'],\n",
    "                               'EndIndex':word_blocks[word_block_index]['end_index'], \n",
    "                               'Confidence':1\n",
    "                              })\n",
    "                        word_block_index = word_block_index + 1 \n",
    "                else : \n",
    "                    result.append({'Entity':word_blocks[word_block_index]['word'], \n",
    "                               'EntityType':value_type, \n",
    "                               'StartIndex':word_blocks[word_block_index]['start_index'],\n",
    "                               'EndIndex':word_blocks[word_block_index]['end_index'], \n",
    "                               'Confidence':1\n",
    "                              })\n",
    "                    word_block_index = word_block_index + 1 \n",
    "\n",
    "            else : \n",
    "                while word_block_index < count_word_blocks : \n",
    "                    word_block = word_blocks[word_block_index]\n",
    "                    word = word_block['word']\n",
    "                    word_start_index = word_block[\"start_index\"]\n",
    "                    word_end_index = word_block[\"end_index\"]\n",
    "                    all_alphanumerics = get_alphanumeric_groups(word)\n",
    "\n",
    "                    if str(value) in all_alphanumerics or str(int(value)) in all_alphanumerics : \n",
    "                        if word_block_index <= count_word_blocks - len(unit.split(\" \")) - 1 : \n",
    "                            next_word_blocks = word_blocks[word_block_index+1:word_block_index+len(unit.split(\" \"))+1]\n",
    "                            unit_word = \" \".join([word_block['word'] for word_block in next_word_blocks])\n",
    "                            if unit_word == unit : \n",
    "                                if len(next_word_blocks) == 0 : \n",
    "                                    end_index = word_end_index\n",
    "                                else : \n",
    "                                    end_index = next_word_blocks[-1]['end_index']\n",
    "                                result.append({'Entity':\" \".join([word, unit]), \n",
    "                                               \"EntityType\":value_type, \n",
    "                                               \"StartIndex\":word_start_index,\n",
    "                                               \"EndIndex\":end_index, \n",
    "                                               \"Confidence\":1\n",
    "                                               })\n",
    "                                word_block_index = word_block_index + len(unit.split(\" \")) + 1 \n",
    "                                break \n",
    "                    else :\n",
    "                        if str(value)+unit in all_alphanumerics  or str(int(value))+unit in all_alphanumerics: \n",
    "                            result.append({'Entity': word, \n",
    "                                           'EntityType':value_type,\n",
    "                                           'StartIndex':word_start_index,\n",
    "                                           'EndIndex':word_end_index, \n",
    "                                           'Confidence':1\n",
    "                                          })\n",
    "                            word_block_index = word_block_index + 1 \n",
    "                            break \n",
    "                    word_block_index = word_block_index + 1\n",
    "\n",
    "    return result\n",
    "\n",
    "import argparse\n",
    "def _process_args():\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('-i', default=r\"D:\\_My_programs\\_CUMC\\Extract_Variables\\_GitHub\\data\\example data diabetes_Type 1.csv\", help='input: a specific disease')\n",
    "    parser.add_argument('-f1', default=r\"D:\\_My_programs\\_CUMC\\Extract_Variables\\_GitHub\\data\\variable_features_dk.csv\", help='input: a feature list')\n",
    "    parser.add_argument('-f2', default=r\"D:\\_My_programs\\_CUMC\\Extract_Variables\\_GitHub\\data\\variable_features_umls.csv\", help='input: a feature list')\n",
    "    parser.add_argument('-v', default=\"HBA1C\", help='Variable name: All, HBA1C, BMI, Glucose, Creatinine, BP-Systolic, BP-Diastolic') # 'All' means to detect all variables\n",
    "    return parser.parse_args(sys.argv[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. arg1 :- Input file\n",
    "2. arg2 :- Variable features. Feel free to add rows according to the domain\n",
    "3. arg3 :- UMLS terms\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.743587Z",
     "start_time": "2020-11-26T11:05:37.451729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-26 16:35:37.923722] found a total of 1350 data items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\am21907\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:46: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2b110dc4964969b1775998a8ac36f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1350.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0\n",
      "processing 1000\n",
      "\n",
      "[2020-11-26 16:36:07.740097] saved processed results into: data/nsclc/output_exp_All_out.csv\n"
     ]
    }
   ],
   "source": [
    "ct_gov_df = pd.read_csv(\"data/nsclc/trail_info.csv\")\n",
    "ct_gov_df = ct_gov_df[['nct_id','eligibility_criteria_textblock','eligibility_criteria_minimum_age',\n",
    "                          'eligibility_criteria_maximum_age','eligibility_criteria_gender']].dropna()\n",
    "out,input_ = extract_variables(ct_gov_df,\n",
    "                               'data/nsclc/output',\n",
    "                 'data/variable_features_dk.csv',\n",
    "                 'data/variable_features_umls.csv',\n",
    "                 'All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.772183Z",
     "start_time": "2020-11-26T11:06:07.746086Z"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(out)\n",
    "out.iloc[:,0].nunique()\n",
    "out.head()\n",
    "extracted_ids = set(out.iloc[:,0].unique().tolist())\n",
    "missed_ids = set(ct_gov_df['nct_id']).difference(extracted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.796220Z",
     "start_time": "2020-11-26T11:06:07.774663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NCT00175578',\n",
       " 'NCT00191841',\n",
       " 'NCT00280202',\n",
       " 'NCT00365963',\n",
       " 'NCT00434668',\n",
       " 'NCT00464282',\n",
       " 'NCT00471978',\n",
       " 'NCT00484016',\n",
       " 'NCT00530205',\n",
       " 'NCT00563160',\n",
       " 'NCT00608868',\n",
       " 'NCT00684385',\n",
       " 'NCT00797238',\n",
       " 'NCT00828022',\n",
       " 'NCT00897117',\n",
       " 'NCT00898417',\n",
       " 'NCT00900172',\n",
       " 'NCT00904514',\n",
       " 'NCT01024062',\n",
       " 'NCT01123460',\n",
       " 'NCT01124669',\n",
       " 'NCT01159288',\n",
       " 'NCT01255150',\n",
       " 'NCT01332240',\n",
       " 'NCT01386203',\n",
       " 'NCT01516983',\n",
       " 'NCT01605916',\n",
       " 'NCT01620853',\n",
       " 'NCT01719536',\n",
       " 'NCT01744925',\n",
       " 'NCT01885754',\n",
       " 'NCT01926171',\n",
       " 'NCT01947062',\n",
       " 'NCT01947868',\n",
       " 'NCT02223611',\n",
       " 'NCT02416726',\n",
       " 'NCT02420405',\n",
       " 'NCT02445924',\n",
       " 'NCT02502240',\n",
       " 'NCT02515760',\n",
       " 'NCT02595450',\n",
       " 'NCT02758054',\n",
       " 'NCT02799862',\n",
       " 'NCT02951897',\n",
       " 'NCT02954991',\n",
       " 'NCT02975752',\n",
       " 'NCT02991924',\n",
       " 'NCT03090815',\n",
       " 'NCT03125603',\n",
       " 'NCT03134534',\n",
       " 'NCT03141957',\n",
       " 'NCT03188562',\n",
       " 'NCT03219970',\n",
       " 'NCT03240250',\n",
       " 'NCT03340506',\n",
       " 'NCT03392506',\n",
       " 'NCT03454685',\n",
       " 'NCT03504098',\n",
       " 'NCT03509779',\n",
       " 'NCT03546452',\n",
       " 'NCT03598296',\n",
       " 'NCT03647098',\n",
       " 'NCT03724500',\n",
       " 'NCT03754530',\n",
       " 'NCT03764917',\n",
       " 'NCT03951012',\n",
       " 'NCT03997799',\n",
       " 'NCT04144569',\n",
       " 'NCT04212481',\n",
       " 'NCT04283773',\n",
       " 'NCT04338867',\n",
       " 'NCT04360655',\n",
       " 'NCT04405661',\n",
       " 'NCT04461652',\n",
       " 'NCT04473703'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.835895Z",
     "start_time": "2020-11-26T11:06:07.798227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n        Inclusion Criteria:\\r\\n\\r\\n          -  Recurrent or progressive Non-Small Cell Lung Cancer stage IV or IIIB patients with\\r\\n             Histologic or cytologic confirmation.\\r\\n\\r\\n          -  Wild type epidermal growth factor receptor status.\\r\\n\\r\\n          -  Progressed after first-line chemotherapy.\\r\\n\\r\\n          -  No previous systemic anticancer therapy.\\r\\n\\r\\n          -  Measurable lesion according to response evaluation criteria in solid tumors with at\\r\\n             least one measurable lesion not previously irradiated.\\r\\n\\r\\n          -  Provision of written informed consent.\\r\\n\\r\\n        Exclusion Criteria:\\r\\n\\r\\n          -  Evidence of clinically active Interstitial Lung Diseases (Patients with chronic,\\r\\n             stable, radiographic changes who are asymptomatic need not be excluded).\\r\\n\\r\\n          -  Positive epidermal growth factor receptor mutation.\\r\\n\\r\\n          -  Known severe hypersensitivity to icotinib or any of the excipients of this product.\\r\\n\\r\\n          -  Evidence of any other significant clinical disorder or laboratory finding that makes\\r\\n             it undesirable for the subject to participate in the study.\\r\\n      '],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_gov_df[ct_gov_df['nct_id']=='NCT01744925']['eligibility_criteria_textblock'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.869987Z",
     "start_time": "2020-11-26T11:06:07.838902Z"
    }
   },
   "outputs": [],
   "source": [
    "# processed_ids = [\"NCT00004984\",\"NCT00005665\",\"NCT00021788\",\"NCT00021801\",\"NCT00034255\",\"NCT00042458\",\"NCT00042471\",\"NCT00042601\",\"NCT00046150\",\"NCT00063128\",\"NCT00071448\",\"NCT00095082\",\"NCT00097292\",\"NCT00100178\",\"NCT00105352\",\"NCT00107107\",\"NCT00108004\",\"NCT00109434\",\"NCT00117026\",\"NCT00117780\",\"NCT00118937\",\"NCT00118976\",\"NCT00119041\",\"NCT00129259\",\"NCT00130481\",\"NCT00131755\",\"NCT00133809\",\"NCT00135915\",\"NCT00140543\",\"NCT00141986\",\"NCT00142922\",\"NCT00143949\",\"NCT00145353\",\"NCT00145379\",\"NCT00146484\",\"NCT00147342\",\"NCT00148538\",\"NCT00160732\",\"NCT00175253\",\"NCT00175266\",\"NCT00179777\",\"NCT00184639\",\"NCT00184665\",\"NCT00187564\",\"NCT00190502\",\"NCT00191581\",\"NCT00198146\",\"NCT00206258\",\"NCT00206297\",\"NCT00206401\",\"NCT00211510\",\"NCT00211536\",\"NCT00212329\",\"NCT00214214\",\"NCT00214253\",\"NCT00223613\",\"NCT00226902\",\"NCT00229658\",\"NCT00239148\",\"NCT00252720\",\"NCT00252733\",\"NCT00254501\",\"NCT00260234\",\"NCT00265473\",\"NCT00271284\",\"NCT00272090\",\"NCT00273286\",\"NCT00276250\",\"NCT00276393\",\"NCT00278980\",\"NCT00279305\",\"NCT00279318\",\"NCT00283218\",\"NCT00284232\",\"NCT00285194\",\"NCT00285233\",\"NCT00286624\",\"NCT00286962\",\"NCT00290979\",\"NCT00291772\",\"NCT00297401\",\"NCT00297583\",\"NCT00297635\",\"NCT00298740\",\"NCT00303134\",\"NCT00304538\",\"NCT00305344\",\"NCT00306098\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.904578Z",
     "start_time": "2020-11-26T11:06:07.873996Z"
    }
   },
   "outputs": [],
   "source": [
    "# text = ct_gov_df[~ct_gov_df['nct_id'].isin(processed_ids)]['eligibility_criteria_textblock'][6]\n",
    "# Valx_core.preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T11:06:07.953206Z",
     "start_time": "2020-11-26T11:06:07.907586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00002520</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>must have smoked 1 or more cigarettes within t...</td>\n",
       "      <td>must have &lt;VL Label=smoked Source=ngram&gt;smoked...</td>\n",
       "      <td>[['smoked', '&gt;=', '1', 'cigarettes']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00002520</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>ecog 0-1</td>\n",
       "      <td>&lt;VL Label=ECOG Source=DK&gt;ecog&lt;/VL&gt; &lt;VML Logic=...</td>\n",
       "      <td>[['ECOG', '&gt;=', 0.0, ''], ['ECOG', '&lt;=', 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00002583</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>10 if complete mediastinal lymph node resectio...</td>\n",
       "      <td>&lt;VML Logic=equal Unit=&gt;10&lt;/VML&gt; if complete me...</td>\n",
       "      <td>[['Lymph node', '=', 10.0, '']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00002583</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>lymph node which measured 1.5 cm or more on pr...</td>\n",
       "      <td>&lt;VL Label=Lymph node Source=DK&gt;lymph node&lt;/VL&gt;...</td>\n",
       "      <td>[['Lymph node', '&gt;=', 1.5, 'cm']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00002583</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>found to be free of metastatic involvement dis...</td>\n",
       "      <td>&lt;VL Label=found to be free of metastatic invol...</td>\n",
       "      <td>[['found to be free of metastatic involvement ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13684</th>\n",
       "      <td>NCT04606303</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>bone marrow hematopoietic function is good, le...</td>\n",
       "      <td>&lt;VL Label=bone marrow hematopoietic function i...</td>\n",
       "      <td>[['bone marrow hematopoietic function is good'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13685</th>\n",
       "      <td>NCT04606303</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>hemoglobin&gt; 10g/dl</td>\n",
       "      <td>&lt;VL Label=hemoglobin Source=ngram&gt;hemoglobin&lt;/...</td>\n",
       "      <td>[['hemoglobin', '&gt;', '10', 'g/dl']]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13686</th>\n",
       "      <td>NCT04606303</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>good renal function, glomerular filtration rat...</td>\n",
       "      <td>good renal function, &lt;VL Label=glomerular filt...</td>\n",
       "      <td>[['glomerular filtration rate', '&gt;', '60', 'ml...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13687</th>\n",
       "      <td>NCT04606303</td>\n",
       "      <td>Inclusion</td>\n",
       "      <td>good liver function, total bilirubin(tbil)&lt;1.5...</td>\n",
       "      <td>good liver function, &lt;VL Label=total bilirubin...</td>\n",
       "      <td>[['total bilirubin level', '&lt;', '1.5', 'uln'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13688</th>\n",
       "      <td>NCT04606303</td>\n",
       "      <td>Exclusion</td>\n",
       "      <td>patients were given antibiotics within 2 weeks</td>\n",
       "      <td>&lt;VL Label=patients were given antibiotics Sour...</td>\n",
       "      <td>[['patients were given antibiotics', '&lt;', '2',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13689 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0          1  \\\n",
       "0      NCT00002520  Inclusion   \n",
       "1      NCT00002520  Inclusion   \n",
       "2      NCT00002583  Inclusion   \n",
       "3      NCT00002583  Inclusion   \n",
       "4      NCT00002583  Inclusion   \n",
       "...            ...        ...   \n",
       "13684  NCT04606303  Inclusion   \n",
       "13685  NCT04606303  Inclusion   \n",
       "13686  NCT04606303  Inclusion   \n",
       "13687  NCT04606303  Inclusion   \n",
       "13688  NCT04606303  Exclusion   \n",
       "\n",
       "                                                       2  \\\n",
       "0      must have smoked 1 or more cigarettes within t...   \n",
       "1                                               ecog 0-1   \n",
       "2      10 if complete mediastinal lymph node resectio...   \n",
       "3      lymph node which measured 1.5 cm or more on pr...   \n",
       "4      found to be free of metastatic involvement dis...   \n",
       "...                                                  ...   \n",
       "13684  bone marrow hematopoietic function is good, le...   \n",
       "13685                                 hemoglobin> 10g/dl   \n",
       "13686  good renal function, glomerular filtration rat...   \n",
       "13687  good liver function, total bilirubin(tbil)<1.5...   \n",
       "13688     patients were given antibiotics within 2 weeks   \n",
       "\n",
       "                                                       3  \\\n",
       "0      must have <VL Label=smoked Source=ngram>smoked...   \n",
       "1      <VL Label=ECOG Source=DK>ecog</VL> <VML Logic=...   \n",
       "2      <VML Logic=equal Unit=>10</VML> if complete me...   \n",
       "3      <VL Label=Lymph node Source=DK>lymph node</VL>...   \n",
       "4      <VL Label=found to be free of metastatic invol...   \n",
       "...                                                  ...   \n",
       "13684  <VL Label=bone marrow hematopoietic function i...   \n",
       "13685  <VL Label=hemoglobin Source=ngram>hemoglobin</...   \n",
       "13686  good renal function, <VL Label=glomerular filt...   \n",
       "13687  good liver function, <VL Label=total bilirubin...   \n",
       "13688  <VL Label=patients were given antibiotics Sour...   \n",
       "\n",
       "                                                       4  \n",
       "0                  [['smoked', '>=', '1', 'cigarettes']]  \n",
       "1      [['ECOG', '>=', 0.0, ''], ['ECOG', '<=', 1.0, ...  \n",
       "2                        [['Lymph node', '=', 10.0, '']]  \n",
       "3                      [['Lymph node', '>=', 1.5, 'cm']]  \n",
       "4      [['found to be free of metastatic involvement ...  \n",
       "...                                                  ...  \n",
       "13684  [['bone marrow hematopoietic function is good'...  \n",
       "13685                [['hemoglobin', '>', '10', 'g/dl']]  \n",
       "13686  [['glomerular filtration rate', '>', '60', 'ml...  \n",
       "13687  [['total bilirubin level', '<', '1.5', 'uln'],...  \n",
       "13688  [['patients were given antibiotics', '<', '2',...  \n",
       "\n",
       "[13689 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
